{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Code\\Python Code\\CT208\\vm\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu gốc: Đó là chưa kể đến hơn 4.600 rebounds và 4.600 hỗ trợ, gần 750 cú steals và 400 pha blocks, những con số có thể gần như là “vô tiền khoáng hậu\". Hơn thế, Lebron James còn 9 lần tham dự All-Star, 9 bình chọn NBA, MVP mùa giải, 5 lần tham dự NBA Finals, 2 chức vô địch NBA, 2 danh hiệu MVP Finals...\n",
      "\n",
      "Câu không dấu: Do la chua ke den hon 4.600 rebounds va 4.600 ho tro, gan 750 cu steals va 400 pha blocks, nhung con so co the gan nhu la \"vo tien khoang hau\". Hon the, Lebron James con 9 lan tham du All-Star, 9 binh chon NBA, MVP mua giai, 5 lan tham du NBA Finals, 2 chuc vo dich NBA, 2 danh hieu MVP Finals...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = input()\n",
    "print(f\"Câu gốc: {sentence}\", end=\"\\n\\n\")\n",
    "sentence = unidecode(sentence)\n",
    "print(f\"Câu không dấu: {sentence}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(\"../3rd/checkpoint-25500\")\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(\"../4th/checkpoint-25500\")\n",
    "model3 = AutoModelForSeq2SeqLM.from_pretrained(\"../5th/checkpoint-34000\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu gốc: \"Đánh giải này, bạn không cần giỏi golf, nhưng kỹ năng trượt tuyết phải ổn\", John Haines, trưởng bộ phận điều phối Snow Golf Tournament chia sẻ trên Golf.com.\n",
      "\n",
      "Câu không dấu: \"Danh giai nay, ban khong can gioi golf, nhung ky nang truot tuyet phai on\", John Haines, truong bo phan dieu phoi Snow Golf Tournament chia se tren Golf.com.\n",
      "\n",
      "model1: \"Dành giải này, bạn không cần giỏi golf, nhưng kỹ năng trượt tuyết phải ổn\", John Haines, trưởng bộ phận điều phối Snow Golf Tournament chia sẻ trên Golf.com.\n",
      "\n",
      "model2: \"Danh giải này, bạn không cần giỏi golf, nhưng kỹ năng trượt tuyết phải ổn\", John Haines, trưởng bộ phận điều phối Snow Golf Tournament chia sẻ trên Golf.com.\n",
      "\n",
      "model3: \"Dành giải này, bạn không cần giỏi golf, nhưng kỹ năng trượt tuyết phải ổn\", John Haines, trưởng bộ phận điều phối Snow Golf Tournament chia sẻ trên Golf.com.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = input()\n",
    "print(f\"Câu gốc: {sentence}\", end=\"\\n\\n\")\n",
    "sentence = unidecode(sentence)\n",
    "print(f\"Câu không dấu: {sentence}\", end=\"\\n\\n\")\n",
    "\n",
    "encoding = tokenizer(sentence, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "output1 = model1.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=1024,\n",
    ")\n",
    "\n",
    "output2 = model2.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=1024,\n",
    ")\n",
    "\n",
    "output3 = model3.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=1024,\n",
    ")\n",
    "\n",
    "for output in output1:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(f\"model1: {line}\", end=\"\\n\\n\")\n",
    "    \n",
    "for output in output2:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(f\"model2: {line}\", end=\"\\n\\n\")\n",
    "    \n",
    "for output in output3:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(f\"model3: {line}\", end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
