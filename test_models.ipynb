{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Code\\Python Code\\CT208\\vm\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(\"./1st/checkpoint-25500\")\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(\"./2nd/checkpoint-25500\")\n",
    "model3 = AutoModelForSeq2SeqLM.from_pretrained(\"./3rd/checkpoint-34000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\"xin chao Viet Nam\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁xin', '▁chao', '▁Viet', '▁Nam', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu gốc: HLV Vũ Văn Trung đánh giá đội Trường ĐH TDTT Bắc Ninh sẽ rất đáng gờm, bởi đây là tập hợp của các cầu thủ sinh viên giàu tố chất thể thao, được ăn tập bài bản, có thể lực rất tốt, kỹ năng chơi bóng ấn tượng.\n",
      "Câu không dấu: HLV Vu Van Trung danh gia doi Truong DH TDTT Bac Ninh se rat dang gom, boi day la tap hop cua cac cau thu sinh vien giau to chat the thao, duoc an tap bai ban, co the luc rat tot, ky nang choi bong an tuong.\n",
      "HLV Vũ Văn Trung đánh giá đội Trường ĐH TDTT Bắc Ninh sẽ rất đáng gờm, bởi đây là tập hợp của các cầu thủ sinh viên giàu tố chất thể thao, được ăn tập bài bản, có thể lực rất tốt, kỹ năng chơi bóng ấn tượng.\n",
      "HLV Vũ Văn Trung đánh giá đội Trường ĐH TDTT Bắc Ninh sẽ rất đáng gờm, bởi đây là tập hợp của các cầu thủ sinh viên giàu tố chất thể thao, được ăn tập bài bản, có thể lực rất tốt, kỹ năng chơi bóng ấn tượng.\n",
      "HLV Vũ Văn Trung đánh giá đội Trường ĐH TDTT Bắc Ninh sẽ rất đáng gờm, bởi đây là tập hợp của các cầu thủ sinh viên giàu tố chất thể thao, được ăn tập bài bản, có thể lực rất tốt, kỹ năng chơi bóng ấn tượng.\n"
     ]
    }
   ],
   "source": [
    "sentence = input()\n",
    "print(f\"Câu gốc: {sentence}\")\n",
    "sentence = unidecode(sentence)\n",
    "print(f\"Câu không dấu: {sentence}\")\n",
    "\n",
    "encoding = tokenizer(sentence, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "output1 = model1.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=1024,\n",
    ")\n",
    "\n",
    "output2 = model2.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=1024,\n",
    ")\n",
    "\n",
    "output3 = model3.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=1024,\n",
    ")\n",
    "\n",
    "for output in output1:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(line)\n",
    "    \n",
    "for output in output2:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(line)\n",
    "    \n",
    "for output in output3:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
